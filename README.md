# Brain-Controlled Prosthetic Gesture Classification
- Created a convolutional neural network to classify EEG signals to control a UR5e robot arm, serving as a brain-controlled prosthetic.
- Collected 1000 EEG samples for each of the four predetermined gestures and a negative classification using an OpenBCI Electrode Cap. Manually verified and filtered EEGs using a Buttersworth filter.
- Initially explored Decision Trees and various machine learning techniques, resulting in accuracies close to random guessing (~20%). Opted for a CNN due to its effectiveness with spatial data.
- Chose accuracy as the performance metric due to balanced classes and safety considerations for the robot arm. Achieved a model accuracy of 82% with the CNN.
- Despite the promising accuracy, the project was complex and prone to errors, particularly during data collection. The CNN's numerous tunable parameters and signal processing steps required meticulous, isolated parameter tuning for optimization.

<div style="text-align:center;">

![Image](/images/BME_CM.png)

</div>

[This Projectâ€™s GitHub Repository](https://github.com/LucinoGarcia/Robot-Arm-Classification)



# Disease Detection with Computer Vision


# Bitcoin Price Regression Prediction


# Clustering and Segmentation for Banking Customers




# In-Progress
### SQL project
### Tableau dashboard
